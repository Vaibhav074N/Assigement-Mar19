{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpki/I1cTQbJ81vb37h6XF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav074N/Assigement-Mar19/blob/main/Assigement_Mar19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application."
      ],
      "metadata": {
        "id": "L31ETj-NUFNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling is a type of feature scaling used in data preprocessing to transform the features of a dataset so that they have a range between 0 and 1. This scaling technique is used to normalize the data and is particularly useful when the feature values have different scales and ranges"
      ],
      "metadata": {
        "id": "1BQYKkAZU4uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for Min-Max scaling is given as:\n",
        "\n",
        "X_scaled = (X - X_min) / (X_max - X_min)\n",
        "\n",
        "where X is the original feature value, X_min is the minimum value of the feature in the dataset, and X_max is the maximum value of the feature in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "-9YyCNrPU-xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling can be applied to each feature of the dataset independently. This scaling technique preserves the distribution of the data and ensures that all features are on the same scale. It is often used as a preprocessing step before applying machine learning algorithms that assume the features are on the same scale, such as k-nearest neighbors, support vector machines, and artificial neural networks."
      ],
      "metadata": {
        "id": "C-3FNALIVS2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfyap7AnUAL6",
        "outputId": "fa1fe319-6214-4156-f6dd-9cd29c7fc40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  ]\n",
            " [0.25]\n",
            " [0.5 ]\n",
            " [0.75]\n",
            " [1.  ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([10, 20, 30, 40, 50])\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to the feature values\n",
        "X_scaled = scaler.fit_transform(X.reshape(-1, 1))\n",
        "\n",
        "print(X_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dY420T7ciQIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application."
      ],
      "metadata": {
        "id": "SfS6eihMUF3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:\n",
        "The Unit Vector technique in feature scaling, also known as L2 normalization or vector normalization, is a method to scale numerical features in a dataset to a unit vector. In this technique, each data point (vector) is divided by its magnitude (Euclidean norm) to ensure that the length of the vector becomes 1. The purpose of this scaling is to make the magnitude of each data point comparable and to prevent features with larger values from dominating the analysis.\n",
        "\n",
        "The formula for calculating the unit vector for each data point is as follows:\n",
        "\n",
        "Unit Vector = Data Point / ||Data Point||\n",
        "\n",
        "Where ||Data Point|| represents the magnitude of the data point, calculated as the square root of the sum of squares of each component of the vector.\n",
        "\n",
        "On the other hand, Min-Max scaling is a different technique that scales the data to a specific range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the range (the difference between the maximum and minimum values)."
      ],
      "metadata": {
        "id": "cSWagOYnh3lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given dataset\n",
        "data_points = np.array([[5, 7], [3, 9], [8, 6]])\n",
        "\n",
        "# Step 1: Unit Vector Scaling\n",
        "magnitudes = np.linalg.norm(data_points, axis=1)  # Calculate magnitudes using Euclidean norm\n",
        "unit_vector_scaled = data_points / magnitudes[:, np.newaxis]  # Divide each data point by its magnitude\n",
        "print(\"Unit Vector Scaled:\")\n",
        "print(unit_vector_scaled)\n",
        "\n",
        "# Step 2: Min-Max Scaling\n",
        "min_values = np.min(data_points, axis=0)\n",
        "max_values = np.max(data_points, axis=0)\n",
        "min_max_scaled = (data_points - min_values) / (max_values - min_values)\n",
        "print(\"Min-Max Scaled:\")\n",
        "print(min_max_scaled)"
      ],
      "metadata": {
        "id": "PSqx5kQVUGfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbee80df-4e5b-4ca0-f3c3-8545a49a17cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit Vector Scaled:\n",
            "[[0.58123819 0.81373347]\n",
            " [0.31622777 0.9486833 ]\n",
            " [0.8        0.6       ]]\n",
            "Min-Max Scaled:\n",
            "[[0.4        0.33333333]\n",
            " [0.         1.        ]\n",
            " [1.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R66GgA7LhKZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application."
      ],
      "metadata": {
        "id": "WV6tZjrnUHGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:\n",
        "PCA, which stands for Principal Component Analysis, is a popular dimensionality reduction technique used in various fields, including machine learning, data analysis, and pattern recognition. It helps in reducing the number of dimensions in a high-dimensional dataset while preserving the most important patterns and variations in the data. This is achieved by transforming the original features into a new set of orthogonal (uncorrelated) features, known as principal components. The first principal component captures the most significant variance in the data, and each subsequent principal component explains as much of the remaining variance as possible.\n",
        "\n",
        "- The steps involved in PCA are as follows:\n",
        "\n",
        "1.Standardize the data: Center the data by subtracting the mean from each feature, and then scale the data to have a variance of 1.\n",
        "\n",
        "2.Calculate the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
        "\n",
        "3.Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
        "\n",
        "4.Choose the number of principal components: Select the top k eigenvectors with the highest eigenvalues to represent the dataset in a lower-dimensional space.\n",
        "\n",
        "5.Project the data onto the new feature space: Transform the original data into the new feature space spanned by the selected principal components.\n",
        "\n",
        "PCA is widely used for dimensionality reduction as it helps in simplifying complex datasets, reduces computation time, and often improves the performance of machine learning models by eliminating noise and redundancy."
      ],
      "metadata": {
        "id": "PsRpCa3GjaDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Given dataset\n",
        "data_points = np.array([[1, 3], [2, 4], [3, 5], [4, 6], [5, 7]])\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "mean = np.mean(data_points, axis=0)\n",
        "std = np.std(data_points, axis=0)\n",
        "centered_data = data_points - mean\n",
        "scaled_data = centered_data / std\n",
        "\n",
        "# Step 2: Perform PCA\n",
        "pca = PCA(n_components=1)  # We want to reduce to 1 dimension\n",
        "reduced_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data_points)\n",
        "print(\"Reduced Data:\")\n",
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b6xtGVjZvj",
        "outputId": "503e2c3a-c726-476d-97d5-746562b853ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[1 3]\n",
            " [2 4]\n",
            " [3 5]\n",
            " [4 6]\n",
            " [5 7]]\n",
            "Reduced Data:\n",
            "[[ 2.]\n",
            " [ 1.]\n",
            " [-0.]\n",
            " [-1.]\n",
            " [-2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnKcRB3WURCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept."
      ],
      "metadata": {
        "id": "yJhCr5-2UUF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "PCA (Principal Component Analysis) is closely related to feature extraction in the context of dimensionality reduction. In fact, PCA can be seen as a feature extraction technique that transforms the original features into a new set of uncorrelated features called principal components. These principal components are linear combinations of the original features and are ranked in order of importance based on the variance they explain in the data.\n",
        "\n",
        "- The main steps of using PCA for feature extraction are:\n",
        "\n",
        "1.Standardize the data: Center the data by subtracting the mean from each feature and scale the data to have a variance of 1.\n",
        "\n",
        "2.Calculate the covariance matrix: Compute the covariance matrix of the standardized data.\n",
        "\n",
        "3.Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
        "\n",
        "4.Choose the number of principal components: Select the top k eigenvectors with the highest eigenvalues to retain the most important information while reducing dimensionality.\n",
        "\n",
        "5.Project the data onto the new feature space: Transform the original data into the new feature space spanned by the selected principal components.\n",
        "\n",
        "The relationship between PCA and feature extraction lies in the fact that PCA extracts new features (principal components) from the original features in a way that maximizes the variance in the data. By doing so, PCA condenses the information in the original features into a smaller set of principal components, allowing for more efficient representation and visualization of the data."
      ],
      "metadata": {
        "id": "I6P2dzmLnpnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data_points = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]])\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "mean = np.mean(data_points, axis=0)\n",
        "std = np.std(data_points, axis=0)\n",
        "centered_data = data_points - mean\n",
        "scaled_data = centered_data / std\n",
        "\n",
        "# Step 2: Perform PCA\n",
        "pca = PCA(n_components=2)  # We want to reduce to 2 dimensions\n",
        "extracted_features = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data_points)\n",
        "print(\"Extracted Features:\")\n",
        "print(extracted_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj4WbpVenoPD",
        "outputId": "881cd239-61da-4e91-950a-370561024d00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[1 2 3]\n",
            " [2 3 4]\n",
            " [3 4 5]\n",
            " [4 5 6]\n",
            " [5 6 7]]\n",
            "Extracted Features:\n",
            "[[ 2.44948974e+00  3.43990023e-16]\n",
            " [ 1.22474487e+00 -1.14663341e-16]\n",
            " [-0.00000000e+00 -0.00000000e+00]\n",
            " [-1.22474487e+00  1.14663341e-16]\n",
            " [-2.44948974e+00  2.29326682e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFRUeXcmnoDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data."
      ],
      "metadata": {
        "id": "IrfX33f8UXaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ans:\n",
        "To preprocess the data for building a recommendation system for a food delivery service, we can use Min-Max scaling to standardize the features. Min-Max scaling will transform the data so that all the features are scaled to a specific range, typically between 0 and 1. This ensures that each feature contributes equally to the recommendation process, regardless of its original scale or magnitude. Here's how Min-Max scaling can be applied to preprocess the data:\n",
        "\n",
        "Gather the dataset: Collect the dataset containing the relevant features for the food items, such as price, rating, and delivery time.\n",
        "\n",
        "Calculate the minimum and maximum values for each feature: Find the minimum and maximum values for each feature in the dataset. These values will be used to perform the scaling.\n",
        "\n",
        "Apply Min-Max scaling: For each feature, apply the Min-Max scaling formula to transform the data into a specific range (e.g., [0, 1]).\n",
        "\n",
        "The Min-Max scaling formula is given by:\n",
        "\n",
        "Scaled Value = (Value - Min) / (Max - Min)\n",
        "\n",
        "where:\n",
        "\n",
        "\"Value\" is the original value of the feature.\n",
        "\n",
        "\"Min\" is the minimum value of the feature in the dataset.\n",
        "\n",
        "\"Max\" is the maximum value of the feature in the dataset.\n",
        "\n",
        "Use the scaled data for the recommendation system: Once the data has been Min-Max scaled, the scaled values for price, rating, and delivery time will all lie between 0 and 1. This ensures that each feature is on the same scale, and no particular feature dominates the recommendation process. The scaled data can then be used in building the recommendation system, such as collaborative filtering or content-based filtering algorithms, to provide personalized food recommendations to users."
      ],
      "metadata": {
        "id": "WcUTnK1_oxBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([\n",
        "    [10, 4.5, 30],\n",
        "    [20, 4.0, 45],\n",
        "    [15, 4.2, 25]\n",
        "])\n",
        "\n",
        "# Step 2: Calculate the minimum and maximum values for each feature\n",
        "min_values = np.min(data, axis=0)\n",
        "max_values = np.max(data, axis=0)\n",
        "\n",
        "# Step 3: Apply Min-Max scaling\n",
        "scaled_data = (data - min_values) / (max_values - min_values)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"Min-Max Scaled Data:\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "trwijhbwUX3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d652aaac-0515-41e0-f735-f5470d86f060"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[10.   4.5 30. ]\n",
            " [20.   4.  45. ]\n",
            " [15.   4.2 25. ]]\n",
            "Min-Max Scaled Data:\n",
            "[[0.   1.   0.25]\n",
            " [1.   0.   1.  ]\n",
            " [0.5  0.4  0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Un8hIH4pQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset."
      ],
      "metadata": {
        "id": "T0mo4iKiUahX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the dimensionality of the dataset for predicting stock prices, you can use PCA (Principal Component Analysis). PCA will help you identify the most important patterns and variations in the data while reducing the number of features (dimensions) to a smaller set of uncorrelated features called principal components. By doing so, PCA simplifies the dataset and makes it more manageable while retaining most of the relevant information for predicting stock prices.\n",
        "\n",
        "- Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
        "\n",
        "1.Standardize the data: Center the data by subtracting the mean from each feature and scale the data to have a variance of 1. Standardization is important for PCA as it ensures that all features are on the same scale and have equal importance in the analysis.\n",
        "\n",
        "2.Calculate the covariance matrix: Compute the covariance matrix of the standardized data. The covariance matrix shows the relationships between different features and helps identify how they vary together.\n",
        "\n",
        "3.Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
        "\n",
        "4.Choose the number of principal components: Select the top k eigenvectors with the highest eigenvalues. The number of principal components you choose will determine the amount of variance retained in the reduced dataset. You can use techniques like explained variance or cumulative explained variance to decide on the number of principal components.\n",
        "\n",
        "5.Project the data onto the new feature space: Transform the original data into the new feature space spanned by the selected principal components. This will result in a lower-dimensional dataset with reduced features.\n",
        "\n",
        "Using PCA for dimensionality reduction can be particularly useful when you have a large number of features, and you want to focus on the most informative ones. It can also help in mitigating the \"curse of dimensionality,\" where high-dimensional datasets can lead to overfitting and increased computational complexity.\n",
        "\n",
        "Once you have the reduced dataset after PCA, you can use it as input for building your model to predict stock prices.\n",
        "\n",
        "Keep in mind that PCA might not always be the best choice, and its effectiveness depends on the specific characteristics of the dataset and the problem at hand. Sometimes, other feature selection or feature engineering techniques might be more appropriate for predicting stock prices. It is essential to experiment with different approaches and evaluate their impact on the model's performance to make an informed decision."
      ],
      "metadata": {
        "id": "HTo5-Jocp82v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Given dataset\n",
        "data = np.array([\n",
        "    [100, 10, 2, 0.8, 50],\n",
        "    [150, 12, 3, 1.2, 55],\n",
        "    [80, 8, 1.5, 0.6, 45],\n",
        "    [120, 11, 2.5, 1.0, 60],\n",
        "    [90, 9, 2, 0.9, 48]\n",
        "])\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Calculate the covariance matrix\n",
        "cov_matrix = np.cov(X_scaled.T)\n",
        "\n",
        "# Step 3: Compute eigenvectors and eigenvalues\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Step 4: Choose the number of principal components\n",
        "n_components = 2\n",
        "top_eigenvectors = eigenvectors[:, :n_components]\n",
        "\n",
        "# Step 5: Project the data onto the new feature space\n",
        "X_reduced = np.dot(X_scaled, top_eigenvectors)\n",
        "\n",
        "# Combine the reduced features (X_reduced) and the target variable (y)\n",
        "reduced_data = np.column_stack((X_reduced, y))\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"PCA Reduced Data:\")\n",
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNAB8x7Wp8PL",
        "outputId": "cd932ea6-6434-442f-fc54-c757d237c5fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[100.   10.    2.    0.8  50. ]\n",
            " [150.   12.    3.    1.2  55. ]\n",
            " [ 80.    8.    1.5   0.6  45. ]\n",
            " [120.   11.    2.5   1.   60. ]\n",
            " [ 90.    9.    2.    0.9  48. ]]\n",
            "PCA Reduced Data:\n",
            "[[ 6.07290513e-01 -3.07910713e-01  5.00000000e+01]\n",
            " [-3.08827623e+00 -1.44731333e-02  5.50000000e+01]\n",
            " [ 2.70653243e+00 -1.79196259e-01  4.50000000e+01]\n",
            " [-1.13970119e+00 -8.21737448e-02  6.00000000e+01]\n",
            " [ 9.14154473e-01  5.83753849e-01  4.80000000e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDU3Ai5-UbAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1."
      ],
      "metadata": {
        "id": "cmZgbzbMUduQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "new_min = -1\n",
        "new_max = 1\n",
        "\n",
        "min_value = np.min(data)\n",
        "max_value = np.max(data)\n",
        "\n",
        "# Apply Min-Max scaling\n",
        "scaled_data = (data - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"Min-Max Scaled Data:\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "eNBPNQffUedT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896509f7-3f0f-4e59-b743-6a9c518e8e69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[ 1  5 10 15 20]\n",
            "Min-Max Scaled Data:\n",
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sU4YrcGt6mm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
        "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
      ],
      "metadata": {
        "id": "z3wgJrrQUi0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:\n",
        "\n",
        "To perform feature extraction using PCA on the given dataset containing features: [height, weight, age, gender, blood pressure], we need to reduce the dimensionality of the data while retaining as much variance as possible. The number of principal components to retain is a crucial decision, as it impacts the amount of information preserved in the reduced dataset.\n",
        "\n",
        "- To determine the number of principal components to retain, we can follow these steps:\n",
        "\n",
        "1.Standardize the data: Center the data by subtracting the mean from each feature and scale the data to have a variance of 1. Standardization is necessary for PCA as it ensures that all features are on the same scale and have equal importance in the analysis.\n",
        "\n",
        "2.Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix shows the relationships between different features and helps identify how they vary together.\n",
        "\n",
        "3.Compute eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.\n",
        "\n",
        "4.Choose the number of principal components: Decide on the number of principal components to retain based on the cumulative explained variance or a threshold percentage of variance explained. For example, you can choose to retain the top k principal components, where k is determined based on the amount of variance you want to retain (e.g., 95% or 99%).\n",
        "\n",
        "5.Project the data onto the new feature space: Transform the original data into the new feature space spanned by the selected principal components.\n",
        "\n",
        "To decide on the number of principal components to retain, we can plot the cumulative explained variance against the number of principal components and visually inspect the point where the curve starts to level off."
      ],
      "metadata": {
        "id": "3kx3UGQJ7T_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Synthetic dataset with 100 samples and 5 features (height, weight, age, gender, blood pressure)\n",
        "np.random.seed(0)\n",
        "height = np.random.normal(loc=170, scale=10, size=100)\n",
        "weight = np.random.normal(loc=70, scale=10, size=100)\n",
        "age = np.random.normal(loc=30, scale=5, size=100)\n",
        "gender = np.random.choice(['Male', 'Female'], size=100)\n",
        "blood_pressure = np.random.normal(loc=120, scale=10, size=100)\n",
        "\n",
        "# Create the dataset by stacking the features horizontally\n",
        "data = np.column_stack((height, weight, age, blood_pressure))\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Step 2: Calculate the covariance matrix\n",
        "cov_matrix = np.cov(data_scaled.T)\n",
        "\n",
        "# Step 3: Compute eigenvectors and eigenvalues\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Step 4: Choose the number of principal components\n",
        "# For this example, let's say we want to retain 95% of the total variance\n",
        "total_variance = np.sum(eigenvalues)\n",
        "explained_variance_ratio = eigenvalues / total_variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
        "\n",
        "# Step 5: Project the data onto the new feature space\n",
        "pca = PCA(n_components=num_components_to_retain)\n",
        "data_reduced = pca.fit_transform(data_scaled)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFyVFyPQ69v2",
        "outputId": "e2095395-93d3-4a07-cfd7-24d1ebe9e730"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[187.64052346  88.83150697  28.15409081 113.15989102]\n",
            " [174.00157208  56.52240939  28.80310411 136.59550796]\n",
            " [179.78737984  57.29515002  35.49829798 130.68509399]\n",
            " [192.40893199  79.69396708  33.27631865 115.46614196]\n",
            " [188.6755799   58.26876595  33.20065763 113.12162389]\n",
            " [160.2272212   89.43621186  21.91521978 107.85922597]\n",
            " [179.50088418  65.86381019  29.87836938 115.59077368]\n",
            " [168.48642792  62.52545189  26.30984545 117.19644505]\n",
            " [168.96781148  89.22942026  31.399623   116.35306456]\n",
            " [174.10598502  84.80514791  29.50924805 121.56703855]\n",
            " [171.44043571  88.6755896   34.55089454 125.78521498]\n",
            " [184.54273507  79.06044658  31.58609108 123.49654457]\n",
            " [177.61037725  61.38774315  33.93163981 112.35856076]\n",
            " [171.21675016  89.10064953  27.66790452 105.62208526]\n",
            " [174.43863233  67.31996629  25.27776872 133.64531848]\n",
            " [173.33674327  78.02456396  27.94975153 113.10550815]\n",
            " [184.94079073  79.47251968  29.91489793 113.477064  ]\n",
            " [167.94841736  68.44989907  31.89575868 114.78810688]\n",
            " [173.13067702  76.1407937   41.29654475 101.5693045 ]\n",
            " [161.45904261  79.22206672  29.78871424 115.22025996]\n",
            " [144.47010184  73.76425531  25.220275   115.20344186]\n",
            " [176.53618595  59.00599209  28.27009112 126.20358298]\n",
            " [178.64436199  72.98238174  27.68202013 126.98457149]\n",
            " [162.5783498   83.26385897  32.40740737 120.03770889]\n",
            " [192.69754624  63.0543214   22.29601493 129.31848374]\n",
            " [155.45634325  68.5036546   30.31630997 123.39964984]\n",
            " [170.45758517  65.64846448  30.78253269 119.84317888]\n",
            " [168.1281615   88.49263728  31.16090518 121.60928168]\n",
            " [185.32779214  76.72294757  27.01341966 118.09346506]\n",
            " [184.6935877   74.07461836  28.81039135 116.05150486]\n",
            " [171.54947426  62.30083926  22.87969546 117.32266463]\n",
            " [173.7816252   75.39249191  27.53340058 108.71988669]\n",
            " [161.12214252  63.25667339  27.28569262 122.80441705]\n",
            " [150.19203532  70.31830558  32.08025023 110.06876389]\n",
            " [166.52087851  63.64153922  24.21908784 128.41631264]\n",
            " [171.56348969  76.76433295  33.90599051 117.5054142 ]\n",
            " [182.30290681  75.76590817  37.47242272 120.49494982]\n",
            " [182.02379849  67.91701244  19.65007487 124.93836776]\n",
            " [166.12673183  73.96006713  32.13129365 126.43314465]\n",
            " [166.97697249  59.06938491  33.38454018 104.29376591]\n",
            " [159.51447035  55.08742407  26.81281487 117.93096324]\n",
            " [155.79982063  74.39391701  28.01364093 128.80178912]\n",
            " [152.93729809  71.66673495  29.33559711 103.01894181]\n",
            " [189.50775395  76.35031437  28.5110456  123.87280475]\n",
            " [164.90347818  93.83144775  28.45493515  97.44435771]\n",
            " [165.61925698  79.44479487  21.61998097 109.77493156]\n",
            " [157.4720464   60.87177775  35.76165782 120.38630552]\n",
            " [177.77490356  81.17016288  35.39809296 103.43284898]\n",
            " [153.86102152  56.84092589  25.9331787  110.14489262]\n",
            " [167.8725972   65.38415395  22.66787836 105.28164993]\n",
            " [161.04533439  69.31758395  32.60532438 136.48134932]\n",
            " [173.86902498  87.13342722  27.12106015 121.64227755]\n",
            " [164.89194862  62.55245178  30.70976582 125.67290278]\n",
            " [158.19367816  61.73561461  28.40335791 117.77324899]\n",
            " [169.71817772  69.01547476  33.45769376 116.46568251]\n",
            " [174.28331871  63.36521714  33.47374572 103.83525811]\n",
            " [170.66517222  81.26635922  26.37201311 117.08162637]\n",
            " [173.02471898  59.20068492  23.08318022 112.38507788]\n",
            " [163.65677906  58.52531348  22.08530801 128.57923924]\n",
            " [166.37258834  65.62179955  33.0518969  131.41101867]\n",
            " [163.27539552  65.01967549  24.05570371 134.66578716]\n",
            " [166.40446838  89.29532054  27.46591823 128.52551939]\n",
            " [161.86853718  79.49420807  27.01842981 114.01346063]\n",
            " [152.73717398  70.87551241  29.73716352 108.84103014]\n",
            " [171.77426142  57.74564481  20.31860097 127.66663182]\n",
            " [165.98219064  78.44362976  30.94389298 123.56292817]\n",
            " [153.69801653  59.99784653  32.61945512 102.31461549]\n",
            " [174.62782256  54.55228903  30.44211044 123.55481793]\n",
            " [160.92701636  81.88029792  28.44556914 128.14519822]\n",
            " [170.51945396  73.16942612  30.48700083 120.58925589]\n",
            " [177.29090562  79.20858824  31.99523173 118.14946329]\n",
            " [171.28982911  73.18727653  16.13703622 111.92351512]\n",
            " [181.39400685  78.56830612  39.77956154 105.534653  ]\n",
            " [157.6517418   63.48974407  31.95046661 128.00297949]\n",
            " [174.02341641  59.65757158  26.73795709 116.90885555]\n",
            " [163.15189909  76.81594518  28.04523312 117.66533338]\n",
            " [161.29202851  61.96590336  32.46870889 137.32721187]\n",
            " [164.21150335  63.10450222  29.4194803  126.84501107]\n",
            " [166.88447468  65.44467496  19.84657766 123.70825001]\n",
            " [170.56165342  70.17479159  40.32246431 121.42061805]\n",
            " [158.34850159  66.46006089  29.44729671 135.19994861]\n",
            " [179.00826487  56.25048707  35.10086356 137.19589307]\n",
            " [174.6566244   63.56381597  26.53975076 129.29505111]\n",
            " [154.63756314  47.76596848  37.68188527 125.82224591]\n",
            " [184.88252194  76.25231451  31.43171844  99.05396929]\n",
            " [188.95889176  53.97942344  33.04421917 121.23721914]\n",
            " [181.78779571  58.95616661  24.77373317 118.69893046]\n",
            " [168.20075164  70.52165079  36.05572645 120.93953229]\n",
            " [159.29247378  62.60437004  33.44909082 129.43046087]\n",
            " [180.54451727  85.43014595  36.50923115  92.60322833]\n",
            " [165.96823053  57.0714309   26.8595622  114.30687947]\n",
            " [182.2244507   72.67050869  27.59486441 122.69904355]\n",
            " [172.08274978  69.60717182  41.51958349 115.33154454]\n",
            " [179.76639036  58.31906502  24.69992089 105.83093887]\n",
            " [173.56366397  75.23276661  29.3202515  128.68963487]\n",
            " [177.06573168  68.28453669  35.68445681 122.76871906]\n",
            " [170.10500021  77.71790551  30.48862484 110.2889543 ]\n",
            " [187.85870494  78.23504154  32.9147684  123.14817205]\n",
            " [171.26912093  91.63235949  28.00275485 128.21585712]\n",
            " [174.01989363  83.36527949  31.85027944 120.05292646]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PCA Reduced Data:\")\n",
        "print(data_reduced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU1_SEKJ7_UU",
        "outputId": "2fcc1c80-fcfd-4370-ce6c-09d2acb60b03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA Reduced Data:\n",
            "[[ 1.95306903e+00 -1.33569470e-01  1.54830281e+00 -3.76928447e-01]\n",
            " [-1.94346653e+00 -1.24627521e+00  4.16267976e-01 -1.81185524e-01]\n",
            " [-8.42943781e-01 -2.17557161e+00 -3.14095293e-01  1.28429850e-01]\n",
            " [ 1.77777136e+00 -1.42611129e+00  9.60429800e-01  4.04710155e-02]\n",
            " [ 4.86907825e-01 -1.68035242e+00  3.07187989e-01  1.56662919e+00]\n",
            " [ 9.30840470e-01  2.61474069e+00  6.41075249e-01 -4.86503358e-01]\n",
            " [ 2.36505486e-01 -5.79327592e-01  3.97873757e-01  7.66880535e-01]\n",
            " [-7.06326067e-01  3.99683339e-01  1.96726935e-01  7.29660493e-01]\n",
            " [ 1.35117719e+00  5.15239146e-01 -7.91921409e-02 -1.13901631e+00]\n",
            " [ 7.93865133e-01  8.33799326e-02  5.51476883e-01 -1.04136207e+00]\n",
            " [ 1.02361022e+00 -4.47854122e-01 -2.36373901e-01 -1.79960188e+00]\n",
            " [ 8.31677260e-01 -1.07592485e+00  8.41934949e-01 -6.13853655e-01]\n",
            " [ 3.91285260e-01 -9.54425252e-01 -4.73466132e-01  1.14149481e+00]\n",
            " [ 1.84389891e+00  1.28880189e+00  4.34059395e-01 -2.18514494e-01]\n",
            " [-1.33125622e+00 -4.07789644e-01  1.11625804e+00 -5.92162843e-01]\n",
            " [ 7.84645080e-01  4.94616536e-01  4.64588401e-01  3.85963020e-02]\n",
            " [ 1.39304419e+00 -4.57753713e-01  9.47438544e-01  1.21214268e-01]\n",
            " [ 1.84960205e-01  3.57146554e-03 -6.18535662e-01  3.33771153e-01]\n",
            " [ 2.34453077e+00 -7.23510346e-01 -1.80378692e+00  6.05043823e-01]\n",
            " [ 4.38012114e-01  9.58305843e-01 -5.18270590e-01 -5.09391691e-01]\n",
            " [-8.09412698e-01  2.44826561e+00 -1.01999644e+00 -3.95918683e-01]\n",
            " [-1.07702097e+00 -8.31340292e-01  5.17788590e-01  4.57168041e-01]\n",
            " [-2.43609881e-01 -5.33686442e-01  1.00564590e+00 -4.75219119e-01]\n",
            " [ 6.00535484e-01  4.70247230e-01 -6.82956356e-01 -1.16916845e+00]\n",
            " [-8.88266628e-01 -1.12352204e+00  2.58795523e+00  4.93745418e-01]\n",
            " [-9.10006357e-01  6.26315598e-01 -1.02883901e+00 -5.14924424e-01]\n",
            " [-3.00093724e-01 -3.07458846e-01 -2.48297032e-01  2.52724737e-01]\n",
            " [ 9.24044899e-01  3.55410665e-01 -1.60732064e-02 -1.47281474e+00]\n",
            " [ 7.38629776e-01 -3.91780620e-01  1.44404642e+00  6.95629606e-02]\n",
            " [ 8.12859608e-01 -5.63750755e-01  1.04810513e+00  3.27928134e-01]\n",
            " [-8.63866868e-01  6.23013093e-01  9.09904175e-01  8.99904576e-01]\n",
            " [ 8.90778403e-01  6.30274597e-01  4.30440240e-01  5.47027911e-01]\n",
            " [-1.20827346e+00  5.25886292e-01 -3.10978539e-01  9.18149041e-02]\n",
            " [-6.04384560e-03  1.35188680e+00 -1.84070229e+00  1.36932856e-01]\n",
            " [-1.57219678e+00  3.41854381e-01  6.06950143e-01 -1.23906059e-01]\n",
            " [ 7.86073072e-01 -3.57957597e-01 -4.92158900e-01 -3.92374978e-01]\n",
            " [ 1.16118031e+00 -1.63452625e+00 -3.03355707e-01 -3.93784024e-01]\n",
            " [-8.71656610e-01  2.00639254e-01  2.30844598e+00  3.09122074e-01]\n",
            " [-2.70826317e-01 -2.48951541e-01 -4.64153907e-01 -9.06959054e-01]\n",
            " [ 3.55934269e-01  6.03846364e-02 -1.26140181e+00  1.64318180e+00]\n",
            " [-1.48387498e+00  6.64062613e-01 -5.76269815e-01  9.62905737e-01]\n",
            " [-1.04750948e+00  8.28322551e-01 -4.57164141e-01 -1.22132203e+00]\n",
            " [ 4.28455025e-01  1.85671451e+00 -1.35210202e+00  6.81820383e-01]\n",
            " [ 5.98234131e-01 -1.09682047e+00  1.58408172e+00 -2.61025550e-01]\n",
            " [ 2.49031081e+00  2.06621532e+00 -1.53405845e-01 -1.23332214e-01]\n",
            " [ 3.66989338e-01  1.95166743e+00  8.88819515e-01  1.80297475e-01]\n",
            " [-7.26666526e-01 -2.66228671e-01 -1.91123372e+00  1.05884957e-01]\n",
            " [ 2.27651299e+00 -2.18533252e-01 -4.91351480e-01  4.02925640e-01]\n",
            " [-1.13875614e+00  1.50821155e+00 -9.15792080e-01  1.29195483e+00]\n",
            " [-4.94778005e-02  1.47599850e+00  5.44221126e-01  1.46577751e+00]\n",
            " [-1.33984983e+00 -5.41662050e-01 -7.65398273e-01 -1.43131171e+00]\n",
            " [ 7.53657664e-01  4.58213569e-01  9.39590057e-01 -1.14248647e+00]\n",
            " [-1.06078104e+00 -2.80033740e-01 -5.46253692e-01 -7.35831830e-02]\n",
            " [-1.00237362e+00  7.41118759e-01 -7.85574734e-01  4.52632977e-01]\n",
            " [ 2.84413178e-01 -3.58050059e-01 -7.00189895e-01  1.73683839e-01]\n",
            " [ 9.08199735e-01 -2.72240758e-01 -7.36120805e-01  1.54811836e+00]\n",
            " [ 5.23059274e-01  7.82586688e-01  6.59923657e-01 -4.76695694e-01]\n",
            " [-6.71150561e-01  6.26573700e-01  8.30271055e-01  1.48478902e+00]\n",
            " [-2.14517457e+00  6.39078497e-01  6.56291843e-01  2.03855421e-01]\n",
            " [-1.02331617e+00 -8.22399010e-01 -6.46772414e-01 -7.17103929e-01]\n",
            " [-2.01262383e+00  3.41446475e-01  5.59383706e-01 -7.26399705e-01]\n",
            " [ 2.09185412e-01  6.55571593e-01  5.69073896e-01 -1.95144591e+00]\n",
            " [ 3.50047588e-01  1.33680292e+00 -9.05540770e-02 -3.56426778e-01]\n",
            " [ 2.94413803e-02  1.55121975e+00 -1.33528172e+00  3.08340830e-01]\n",
            " [-1.97605023e+00  3.64408314e-01  1.41521235e+00  5.52291660e-01]\n",
            " [ 9.67555124e-02  1.54765493e-01 -2.65716153e-01 -9.76443627e-01]\n",
            " [ 2.15351294e-02  1.10285597e+00 -2.01963995e+00  1.44313742e+00]\n",
            " [-1.09189851e+00 -9.94535498e-01 -5.98794715e-02  8.40876104e-01]\n",
            " [-3.40019024e-01  6.87797956e-01 -7.05136239e-02 -1.57569752e+00]\n",
            " [ 9.16018489e-02 -9.60132072e-02 -5.21288562e-02 -2.96709196e-01]\n",
            " [ 9.58466369e-01 -4.39654933e-01  2.20098937e-01 -4.20637663e-01]\n",
            " [-3.41593187e-01  2.01290358e+00  2.00832835e+00  7.29477203e-01]\n",
            " [ 2.41990218e+00 -1.15503640e+00 -9.28448628e-01  3.89468370e-01]\n",
            " [-1.31753207e+00 -5.05102375e-02 -1.14106901e+00 -4.98404495e-01]\n",
            " [-6.39137378e-01 -7.08716493e-02  4.32202048e-01  1.05564044e+00]\n",
            " [ 7.08384044e-02  8.98694014e-01 -1.45211131e-01 -4.33619446e-01]\n",
            " [-1.84282189e+00 -7.81145171e-01 -8.44899693e-01 -9.86756484e-01]\n",
            " [-1.21712063e+00 -1.10175912e-01 -3.64424137e-01 -1.73009157e-01]\n",
            " [-1.45799661e+00  1.11384269e+00  1.23806655e+00  2.16640651e-01]\n",
            " [ 5.53330319e-01 -1.44493851e+00 -1.57250120e+00 -4.25757135e-01]\n",
            " [-1.74951784e+00 -3.08884568e-03 -5.36294772e-01 -1.12144948e+00]\n",
            " [-1.37816038e+00 -2.38099414e+00 -2.06112468e-01 -2.67464059e-01]\n",
            " [-1.18503760e+00 -5.00167443e-01  7.94753864e-01 -6.22191383e-02]\n",
            " [-1.83563023e+00 -9.22618964e-01 -2.51987185e+00  4.87382726e-01]\n",
            " [ 2.22431727e+00 -1.24105478e-01  3.98568623e-01  1.31305275e+00]\n",
            " [-2.94442654e-01 -2.14122631e+00  4.18076292e-01  1.29306059e+00]\n",
            " [-6.64339942e-01 -4.10589088e-01  1.24808145e+00  1.20550954e+00]\n",
            " [ 2.20975247e-01 -7.33516745e-01 -1.08289342e+00 -3.49823874e-01]\n",
            " [-1.29945641e+00 -4.26037879e-01 -1.25184790e+00 -5.44292058e-01]\n",
            " [ 3.40367342e+00  4.44594005e-02 -5.99027444e-01  9.12253413e-01]\n",
            " [-9.02796388e-01  4.59668097e-01 -1.97493927e-01  1.22902221e+00]\n",
            " [ 1.30153405e-01 -5.76294476e-01  1.16682112e+00 -6.87144552e-02]\n",
            " [ 1.04583661e+00 -1.44894285e+00 -1.77462936e+00  4.33950076e-02]\n",
            " [ 4.38215187e-02  2.51826220e-01  8.86916530e-01  2.11363735e+00]\n",
            " [-2.76931598e-01 -4.26913870e-01  5.02056161e-01 -9.06793902e-01]\n",
            " [ 2.50825334e-01 -1.38598624e+00 -4.63307664e-01 -1.18783763e-01]\n",
            " [ 1.01289074e+00  4.91963700e-01 -1.82655305e-01  1.15026293e-01]\n",
            " [ 1.01338140e+00 -1.45921352e+00  8.33861351e-01 -4.96007206e-01]\n",
            " [ 5.79078416e-01  3.60095738e-01  8.37333336e-01 -1.99245589e+00]\n",
            " [ 9.65577891e-01 -1.79617326e-01  1.39728433e-01 -9.04402041e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Principal Components Retained:\", num_components_to_retain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t7SwdR68Wxy",
        "outputId": "586f80e6-bc4b-49c5-eeee-f6352de1bd90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Principal Components Retained: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Explained Variance Ratio:\", explained_variance_ratio[:num_components_to_retain])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmUhoHMy8dUp",
        "outputId": "e92363cd-b75f-40d3-c4d8-d0a3f46799e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.33499486 0.18169156 0.23433278 0.24898081]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cumulative Explained Variance:\", cumulative_explained_variance[:num_components_to_retain])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOD7eA2h8fQR",
        "outputId": "acddb9ff-c37e-451b-f7c1-d0f002e4a46d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Explained Variance: [0.33499486 0.51668642 0.75101919 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlR4HTdT8fvt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_y8bmSJ9Tfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}